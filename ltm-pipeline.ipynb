{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pipeline to retrieve books from predicted date of a query text reranked by similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "#imports\n",
    "\n",
    "import pandas as pd\n",
    "pd.options.mode.chained_assignment = None \n",
    "import torch\n",
    "import numpy as np\n",
    "from simpletransformers.classification import ClassificationModel, ClassificationArgs\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from nltk.tokenize import word_tokenize\n",
    "import gensim\n",
    "from gensim.models.doc2vec import TaggedDocument"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Read the dataset from our preprocessed json file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_json('gutenberg-dataset-v2.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Selecting our samples from the dataset and split it into train and test subsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>author</th>\n",
       "      <th>date</th>\n",
       "      <th>text_ratio</th>\n",
       "      <th>text</th>\n",
       "      <th>text_len_characters</th>\n",
       "      <th>weights</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>18261</th>\n",
       "      <td>The Piazza Tales</td>\n",
       "      <td>Herman Melville</td>\n",
       "      <td>1856</td>\n",
       "      <td>0.962312</td>\n",
       "      <td>huskily continued don benito painfully turning...</td>\n",
       "      <td>11497</td>\n",
       "      <td>0.025641</td>\n",
       "      <td>-0.28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38208</th>\n",
       "      <td>The Spell of Egypt</td>\n",
       "      <td>Robert Hichens</td>\n",
       "      <td>1911</td>\n",
       "      <td>0.970044</td>\n",
       "      <td>in the statue she is presented to us as a lime...</td>\n",
       "      <td>10691</td>\n",
       "      <td>0.071429</td>\n",
       "      <td>0.82</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    title           author  date  text_ratio  \\\n",
       "18261    The Piazza Tales  Herman Melville  1856    0.962312   \n",
       "38208  The Spell of Egypt   Robert Hichens  1911    0.970044   \n",
       "\n",
       "                                                    text  text_len_characters  \\\n",
       "18261  huskily continued don benito painfully turning...                11497   \n",
       "38208  in the statue she is presented to us as a lime...                10691   \n",
       "\n",
       "        weights  labels  \n",
       "18261  0.025641   -0.28  \n",
       "38208  0.071429    0.82  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df[df['date'] >= 1820]\n",
    "df = df[df['date'] <= 1920]\n",
    "\n",
    "earliest_date = df['date'].min()\n",
    "latest_date = df['date'].max()\n",
    "\n",
    "samples_list = []\n",
    "for year in range(earliest_date, latest_date + 1):\n",
    "    df_year = df[df['date'] == year]\n",
    "    if len(df_year) > 100:\n",
    "        samples = df_year.sample(100, random_state=42)\n",
    "        samples_list.append(samples)\n",
    "    else:\n",
    "        samples_list.append(df_year)\n",
    "\n",
    "df_samples = pd.concat(samples_list)\n",
    "df_samples['labels'] = df_samples['date'].apply(lambda date: np.interp(date, [earliest_date, latest_date], [-1, 1]))\n",
    "\n",
    "# Split data into train and test subsets\n",
    "df_train, df_test = train_test_split(df_samples, test_size=0.2, random_state=42, shuffle=True)\n",
    "df_test = df_test.head(2)\n",
    "df_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Setup and load the pretrained classification model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "cuda_available = torch.cuda.is_available()\n",
    "model_args = ClassificationArgs()\n",
    "model_args.reprocess_input_data = True\n",
    "model_args.overwrite_output_dir = True\n",
    "model_args.regression = True\n",
    "\n",
    "# Create a ClassificationModel\n",
    "bert_model = ClassificationModel(\n",
    "    'bert',\n",
    "    'bert-transformer/bert-base-historic-english-cased/outputs', # Load our own pre-trained model\n",
    "    num_labels=1,\n",
    "    args=model_args,\n",
    "    use_cuda=cuda_available\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Set up and train the Doc2Vec model for the re-ranking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_vectorize_docs(train_df, vector_size=40, min_count=4, epochs=30):\n",
    "    \"\"\"\n",
    "    Train a Doc2Vec model on a given DataFrame containing textual data and date information.\n",
    "\n",
    "    Parameters:\n",
    "    - train_df (pandas.DataFrame): Input DataFrame with 'text' and 'date' columns.\n",
    "    - vector_size (int): Dimensionality of the document vectors in the Doc2Vec model.\n",
    "    - min_count (int): Ignores all words with a total frequency lower than this.\n",
    "    - epochs (int): Number of iterations over the entire dataset during training.\n",
    "\n",
    "    Returns:\n",
    "    - pandas.DataFrame: DataFrame containing vectors, original text, date ranges, and other information.\n",
    "    \"\"\"\n",
    "\n",
    "    # Tokenization of each snippet\n",
    "    train_df['tokenized_text'] = train_df['text'].apply(word_tokenize)\n",
    "\n",
    "    new_df = train_df.groupby('title')['tokenized_text'].agg(lambda x: sum(x, [])).reset_index()# Grouping books else we have multile instaces of same book in our rerank\n",
    "    # Preptaring the data for the Doc2Vec model\n",
    "    train_corpus = []\n",
    "    for i, words in enumerate(new_df['tokenized_text']):\n",
    "        tagged_doc = TaggedDocument(words=words, tags=[str(i)])\n",
    "        train_corpus.append(tagged_doc)\n",
    "\n",
    "    # Train the Doc2Vec model\n",
    "    model = gensim.models.Doc2Vec(vector_size=vector_size, min_count=min_count, epochs=epochs)\n",
    "    model.build_vocab(train_corpus)\n",
    "    model.train(train_corpus, total_examples=model.corpus_count, epochs=model.epochs)\n",
    "\n",
    "    # Infer Vectors and save in column (maybe usfull if we want to use them for further processing)\n",
    "    vectors = [model.infer_vector(words) for words in new_df['tokenized_text']]\n",
    "    new_df['vectors'] = vectors\n",
    "\n",
    "    return new_df, model\n",
    "\n",
    "def get_most_similar_books(query, model, df_trained, top_results = 10):\n",
    "    df_query = pd.DataFrame({'text': [query]})\n",
    "    df_query['tokenized_text'] = df_query['text'].apply(word_tokenize)\n",
    "\n",
    "    vector = model.infer_vector(df_query['tokenized_text'].iloc[0])\n",
    "    similar_docs = model.dv.most_similar(vector, topn=top_results)\n",
    "\n",
    "    data = {'similarity': [], 'title': []} #, 'author': [], 'date': []}\n",
    "    for doc in similar_docs:\n",
    "        data['title'].append(df_trained.iloc[int(doc[0])]['title'])\n",
    "        #data['author'].append(df_trained.iloc[int(doc[0])]['author'])\n",
    "        #data['date'].append(df_trained.iloc[int(doc[0])]['date'])\n",
    "        data['similarity'].append(round(doc[1], 3))\n",
    "\n",
    "    return pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Helper methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_date_from_prediction(prediction):\n",
    "    return int(np.interp(prediction, [-1, 1], [earliest_date, latest_date]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create predictions and retrieve book of the predicted year, then re-rank them by similarity to the query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/2 [00:00<?, ?it/s]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      " 50%|█████     | 1/2 [00:04<00:04,  4.56s/it]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.23s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most similar to query \"huskily continued don benito painfully turning in the half embrace of his servant i have to thank th...\":\n",
      "Similarity: 0.624 | Title: Jack Harkaway in New York; or, The Adventures of the Travelers' Club by | Publish Date: \n",
      "Similarity: 0.609 | Title: Harper's Young People, November 11, 1879 / An Illustrated Weekly by | Publish Date: \n",
      "Similarity: 0.585 | Title: Harper's Young People, November 18, 1879 / An Illustrated Weekly by | Publish Date: \n",
      "Similarity: 0.57 | Title: The Royal Regiment, and Other Novelettes by | Publish Date: \n",
      "Similarity: 0.537 | Title: The Campaigns of the British Army at Washington and New Orleans 1814-1815 by | Publish Date: \n",
      "Similarity: 0.509 | Title: Andersonville: A Story of Rebel Military Prisons — Volume 2 by | Publish Date: \n",
      "Similarity: 0.476 | Title: Andersonville: A Story of Rebel Military Prisons — Volume 4 by | Publish Date: \n",
      "Similarity: 0.454 | Title: Harper's Young People, November 4, 1879 / An Illustrated Weekly by | Publish Date: \n",
      "Similarity: 0.414 | Title: Harper's Young People, December 9, 1879 / An Illustrated Weekly by | Publish Date: \n",
      "Similarity: 0.409 | Title: Harper's Young People, December 16, 1879 / An Illustrated Weekly by | Publish Date: \n",
      "Most similar to query \"in the statue she is presented to us as a limestone cow and positively this cow is to be worshipped ...\":\n",
      "Similarity: 0.647 | Title: A Tour Through the Pyrenees by | Publish Date: \n",
      "Similarity: 0.592 | Title: The Catholic World, Vol. 20, October 1874‐March 1875 by | Publish Date: \n",
      "Similarity: 0.581 | Title: The Nursery, Volume 17, No. 100, April, 1875 / A Monthly Magazine for Youngest Readers by | Publish Date: \n",
      "Similarity: 0.579 | Title: Grand'ther Baldwin's Thanksgiving, with Other Ballads and Poems by | Publish Date: \n",
      "Similarity: 0.562 | Title: The Nursery, Volume 17, No. 101, May, 1875 / A Monthly Magazine for Youngest Readers by | Publish Date: \n",
      "Similarity: 0.478 | Title: Essays Æsthetical by | Publish Date: \n",
      "Similarity: 0.462 | Title: Olive: A Novel by | Publish Date: \n",
      "Similarity: 0.46 | Title: The Story of Frithiof the Bold / 1875 by | Publish Date: \n",
      "Similarity: 0.407 | Title: A Strange World: A Novel. Volume 2 (of 3) by | Publish Date: \n",
      "Similarity: 0.388 | Title: Cremation of the Dead: Its History and Bearings Upon Public Health by | Publish Date: \n"
     ]
    }
   ],
   "source": [
    "query_texts = df_test['text'].tolist()\n",
    "query_real_dates = df_test['date'].tolist()\n",
    "predictions, raw_outputs = bert_model.predict(query_texts)\n",
    "prediction_dates = [get_date_from_prediction(pred) for pred in predictions]\n",
    "\n",
    "for query, pred_date in zip(query_texts, prediction_dates):\n",
    "    df_books_from_pred_date = df[df['date'] == pred_date]    # Includes up to 1500 samples\n",
    "\n",
    "    df_trained, doc2vec_model = train_and_vectorize_docs(df_books_from_pred_date)\n",
    "    similar_docs = get_most_similar_books(query, doc2vec_model, df_trained, top_results=10)\n",
    "\n",
    "    print(f\"Most similar to query \\\"{query[:100]}...\\\":\")\n",
    "    similar_docs.apply(lambda doc: print(f\"Similarity: {doc['similarity']} | Title: {doc['title']} by | Publish Date: \"), axis=1)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
