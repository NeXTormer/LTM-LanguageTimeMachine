{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pipeline to retrieve books from predicted date of a query text reranked by similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#imports\n",
    "\n",
    "import pandas as pd\n",
    "pd.options.mode.chained_assignment = None \n",
    "import torch\n",
    "import numpy as np\n",
    "from simpletransformers.classification import ClassificationModel, ClassificationArgs\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from nltk.tokenize import word_tokenize\n",
    "import gensim\n",
    "from gensim.models.doc2vec import TaggedDocument"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Read the dataset from our preprocessed json file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_json('gutenberg-dataset-v2.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Selecting our samples from the dataset and split it into train and test subsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>author</th>\n",
       "      <th>date</th>\n",
       "      <th>text_ratio</th>\n",
       "      <th>text</th>\n",
       "      <th>text_len_characters</th>\n",
       "      <th>weights</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>172381</th>\n",
       "      <td>Voces Populi</td>\n",
       "      <td>F. Anstey</td>\n",
       "      <td>1892</td>\n",
       "      <td>0.930760</td>\n",
       "      <td>known to some of you i dare say as the throstl...</td>\n",
       "      <td>10407</td>\n",
       "      <td>0.058824</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>209579</th>\n",
       "      <td>The Heart of the White Mountains, Their Legend...</td>\n",
       "      <td>Samuel Adams Drake</td>\n",
       "      <td>1882</td>\n",
       "      <td>0.965637</td>\n",
       "      <td>at this point and passes over a long stretch o...</td>\n",
       "      <td>11967</td>\n",
       "      <td>0.016949</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>188083</th>\n",
       "      <td>The Ceramic Art / A Compendium of The History ...</td>\n",
       "      <td>Jennie J. Young</td>\n",
       "      <td>1878</td>\n",
       "      <td>0.951185</td>\n",
       "      <td>in edinburgh scotland and first worked in the ...</td>\n",
       "      <td>12002</td>\n",
       "      <td>0.013889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81657</th>\n",
       "      <td>At Good Old Siwash</td>\n",
       "      <td>George Fitch</td>\n",
       "      <td>1916</td>\n",
       "      <td>0.970252</td>\n",
       "      <td>sitting on the front porch and guarding their ...</td>\n",
       "      <td>10394</td>\n",
       "      <td>0.027778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>158316</th>\n",
       "      <td>The Lion's Brood</td>\n",
       "      <td>Duffield Osborne</td>\n",
       "      <td>1904</td>\n",
       "      <td>0.964153</td>\n",
       "      <td>compel duty a look of cunning crossed his face...</td>\n",
       "      <td>10979</td>\n",
       "      <td>0.034483</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149787</th>\n",
       "      <td>The Song of Hiawatha: An Epic Poem</td>\n",
       "      <td>Henry Wadsworth Longfellow</td>\n",
       "      <td>1898</td>\n",
       "      <td>0.946441</td>\n",
       "      <td>arrows only paused to rest beneath a pinetree ...</td>\n",
       "      <td>11125</td>\n",
       "      <td>0.055556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78923</th>\n",
       "      <td>Klytia: A Story of Heidelberg Castle</td>\n",
       "      <td>Adolf Hausrath</td>\n",
       "      <td>1883</td>\n",
       "      <td>0.970222</td>\n",
       "      <td>whether the greek father of the gods was about...</td>\n",
       "      <td>10909</td>\n",
       "      <td>0.020833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8358</th>\n",
       "      <td>A Handbook of the Boer War / With General Map ...</td>\n",
       "      <td></td>\n",
       "      <td>1910</td>\n",
       "      <td>0.964687</td>\n",
       "      <td>it showed the capture of an armoured train on ...</td>\n",
       "      <td>11568</td>\n",
       "      <td>0.018868</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>273262</th>\n",
       "      <td>Samantha Among the Colored Folks: \"My Ideas on...</td>\n",
       "      <td>Marietta Holley</td>\n",
       "      <td>1894</td>\n",
       "      <td>0.961732</td>\n",
       "      <td>on with that same plaintive sweet song and it ...</td>\n",
       "      <td>10246</td>\n",
       "      <td>0.023256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>167316</th>\n",
       "      <td>The Inferno</td>\n",
       "      <td>August Strindberg</td>\n",
       "      <td>1913</td>\n",
       "      <td>0.966398</td>\n",
       "      <td>and the rest of the furniture moves whenever i...</td>\n",
       "      <td>10810</td>\n",
       "      <td>0.062500</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows Ã— 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    title  \\\n",
       "172381                                       Voces Populi   \n",
       "209579  The Heart of the White Mountains, Their Legend...   \n",
       "188083  The Ceramic Art / A Compendium of The History ...   \n",
       "81657                                  At Good Old Siwash   \n",
       "158316                                   The Lion's Brood   \n",
       "...                                                   ...   \n",
       "149787                 The Song of Hiawatha: An Epic Poem   \n",
       "78923                Klytia: A Story of Heidelberg Castle   \n",
       "8358    A Handbook of the Boer War / With General Map ...   \n",
       "273262  Samantha Among the Colored Folks: \"My Ideas on...   \n",
       "167316                                        The Inferno   \n",
       "\n",
       "                            author  date  text_ratio  \\\n",
       "172381                   F. Anstey  1892    0.930760   \n",
       "209579          Samuel Adams Drake  1882    0.965637   \n",
       "188083             Jennie J. Young  1878    0.951185   \n",
       "81657                 George Fitch  1916    0.970252   \n",
       "158316            Duffield Osborne  1904    0.964153   \n",
       "...                            ...   ...         ...   \n",
       "149787  Henry Wadsworth Longfellow  1898    0.946441   \n",
       "78923               Adolf Hausrath  1883    0.970222   \n",
       "8358                                1910    0.964687   \n",
       "273262             Marietta Holley  1894    0.961732   \n",
       "167316           August Strindberg  1913    0.966398   \n",
       "\n",
       "                                                     text  \\\n",
       "172381  known to some of you i dare say as the throstl...   \n",
       "209579  at this point and passes over a long stretch o...   \n",
       "188083  in edinburgh scotland and first worked in the ...   \n",
       "81657   sitting on the front porch and guarding their ...   \n",
       "158316  compel duty a look of cunning crossed his face...   \n",
       "...                                                   ...   \n",
       "149787  arrows only paused to rest beneath a pinetree ...   \n",
       "78923   whether the greek father of the gods was about...   \n",
       "8358    it showed the capture of an armoured train on ...   \n",
       "273262  on with that same plaintive sweet song and it ...   \n",
       "167316  and the rest of the furniture moves whenever i...   \n",
       "\n",
       "        text_len_characters   weights  \n",
       "172381                10407  0.058824  \n",
       "209579                11967  0.016949  \n",
       "188083                12002  0.013889  \n",
       "81657                 10394  0.027778  \n",
       "158316                10979  0.034483  \n",
       "...                     ...       ...  \n",
       "149787                11125  0.055556  \n",
       "78923                 10909  0.020833  \n",
       "8358                  11568  0.018868  \n",
       "273262                10246  0.023256  \n",
       "167316                10810  0.062500  \n",
       "\n",
       "[100 rows x 7 columns]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We only want to use the books that have a date between 1820 and 1920\n",
    "df = df[df['date'] >= 1820]\n",
    "df = df[df['date'] <= 1920]\n",
    "\n",
    "earliest_date = df['date'].min()\n",
    "latest_date = df['date'].max()\n",
    "\n",
    "# Split data into train and test subsets\n",
    "df_train, df_test = train_test_split(df, test_size=0.1, random_state=42, shuffle=True)\n",
    "\n",
    "df_test = df_test.head(5)\n",
    "df_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Setup and load the pretrained classification model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "cuda_available = torch.cuda.is_available()\n",
    "model_args = ClassificationArgs()\n",
    "model_args.reprocess_input_data = True\n",
    "model_args.overwrite_output_dir = True\n",
    "model_args.regression = True\n",
    "\n",
    "# Create a ClassificationModel\n",
    "bert_model = ClassificationModel(\n",
    "    'bert',\n",
    "    'bert-transformer/bert-base-historic-english-cased/outputs', # Load our own pre-trained model\n",
    "    num_labels=1,\n",
    "    args=model_args,\n",
    "    use_cuda=cuda_available\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Set up and train the Doc2Vec model for the re-ranking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_vectorize_docs(train_df, vector_size=40, min_count=4, epochs=30):\n",
    "    \"\"\"\n",
    "    Train a Doc2Vec model on a given DataFrame containing textual data and date information.\n",
    "\n",
    "    Parameters:\n",
    "    - train_df (pandas.DataFrame): Input DataFrame with 'text' and 'date' columns.\n",
    "    - vector_size (int): Dimensionality of the document vectors in the Doc2Vec model.\n",
    "    - min_count (int): Ignores all words with a total frequency lower than this.\n",
    "    - epochs (int): Number of iterations over the entire dataset during training.\n",
    "\n",
    "    Returns:\n",
    "    - pandas.DataFrame: DataFrame containing vectors, original text, date ranges, and other information.\n",
    "    \"\"\"\n",
    "\n",
    "    # Tokenization of each snippet\n",
    "    train_df['tokenized_text'] = train_df['text'].apply(word_tokenize)\n",
    "\n",
    "    new_df = train_df.groupby('title')['tokenized_text'].agg(lambda x: sum(x, [])).reset_index()# Grouping books else we have multile instaces of same book in our rerank\n",
    "    # Preptaring the data for the Doc2Vec model\n",
    "    train_corpus = []\n",
    "    for i, words in enumerate(new_df['tokenized_text']):\n",
    "        tagged_doc = TaggedDocument(words=words, tags=[str(i)])\n",
    "        train_corpus.append(tagged_doc)\n",
    "\n",
    "    # Train the Doc2Vec model\n",
    "    model = gensim.models.Doc2Vec(vector_size=vector_size, min_count=min_count, epochs=epochs)\n",
    "    model.build_vocab(train_corpus)\n",
    "    model.train(train_corpus, total_examples=model.corpus_count, epochs=model.epochs)\n",
    "\n",
    "    # Infer Vectors and save in column (maybe usfull if we want to use them for further processing)\n",
    "    vectors = [model.infer_vector(words) for words in new_df['tokenized_text']]\n",
    "    new_df['vectors'] = vectors\n",
    "\n",
    "    return new_df, model\n",
    "\n",
    "def get_most_similar_books(query, model, df_trained, top_results = 10):\n",
    "    \"\"\"\n",
    "    Retrieves the most similar books to a given query using a trained model.\n",
    "\n",
    "    Parameters:\n",
    "    query (str): The query text.\n",
    "    model (gensim.models.doc2vec.Doc2Vec): The trained Doc2Vec model.\n",
    "    df_trained (pandas.DataFrame): The DataFrame containing the trained data.\n",
    "    top_results (int, optional): The number of top results to retrieve. Defaults to 10.\n",
    "\n",
    "    Returns:\n",
    "    pandas.DataFrame: A DataFrame containing the most similar books with their similarity scores, titles, authors, and dates.\n",
    "    \"\"\"\n",
    "    df_query = pd.DataFrame({'text': [query]})\n",
    "    df_query['tokenized_text'] = df_query['text'].apply(word_tokenize)\n",
    "\n",
    "    vector = model.infer_vector(df_query['tokenized_text'].iloc[0])\n",
    "    similar_docs = model.dv.most_similar(vector, topn=top_results)\n",
    "\n",
    "    data = {'similarity': [], 'title': [], 'author': [], 'date': []}\n",
    "    for doc in similar_docs:\n",
    "        data['title'].append(df_trained.iloc[int(doc[0])]['title'])\n",
    "        data['author'].append(df_train[df_train['title'] == df_trained.iloc[int(doc[0])]['title']]['author'].iloc[0])\n",
    "        data['date'].append(df_train[df_train['title'] == df_trained.iloc[int(doc[0])]['title']]['date'].iloc[0])\n",
    "        data['similarity'].append(round(doc[1], 3))\n",
    "\n",
    "    return pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Helper methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_date_from_prediction(prediction):\n",
    "    \"\"\"\n",
    "    Convert a prediction value to a date.\n",
    "\n",
    "    Parameters:\n",
    "    - prediction (float): Prediction value between -1 and 1.\n",
    "\n",
    "    Returns:\n",
    "    - int: Date (year between earliest_date and latest_date)\n",
    "    \"\"\"\n",
    "    return int(np.interp(prediction, [-1, 1], [earliest_date, latest_date]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create predictions and retrieve book of the predicted year, then re-rank them by similarity to the query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/5 [00:00<?, ?it/s]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      " 20%|â–ˆâ–ˆ        | 1/5 [00:04<00:17,  4.34s/it]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:01<00:00,  1.08s/it]\n"
     ]
    }
   ],
   "source": [
    "query_texts = df_test['text'].tolist()\n",
    "query_real_dates = df_test['date'].tolist()\n",
    "# Get predictions for query texts from the BERT model\n",
    "predictions, raw_outputs = bert_model.predict(query_texts)\n",
    "# Convert predictions back to dates\n",
    "prediction_dates = [get_date_from_prediction(pred) for pred in predictions]\n",
    "\n",
    "# Get at least 10 books for each query and pass them to the Doc2Vec model\n",
    "for query, pred_date in zip(query_texts, prediction_dates):\n",
    "    df_books_pred = df_train[df_train['date'] == pred_date]\n",
    "    distance_from_pred_date = 1\n",
    "    while len(df_books_pred['title'].unique()) < 10:\n",
    "        # Add books from the previous and following years until we have at least 10 books\n",
    "        df_books_pred = pd.concat([df_books_pred, df_train[df_train['date'] == (pred_date + distance_from_pred_date)]])\n",
    "        df_books_pred = pd.concat([df_books_pred, df_train[df_train['date'] == (pred_date - distance_from_pred_date)]])\n",
    "        distance_from_pred_date += 1\n",
    "\n",
    "    # Train the Doc2Vec model and get a list of the most similar books\n",
    "    df_trained, doc2vec_model = train_and_vectorize_docs(df_books_pred)\n",
    "    similar_docs = get_most_similar_books(query, doc2vec_model, df_trained, top_results=10)\n",
    "\n",
    "    print(f\"Most similar to query \\\"{query[:100]}...\\\":\")\n",
    "    similar_docs.apply(lambda doc: print(f\"Similarity: {doc['similarity']} | Title: {doc['title']} |\",\n",
    "                                         f\"Author: {doc['author']} | Publish Date: {doc['date']}\"), axis=1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
