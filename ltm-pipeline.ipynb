{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pipeline to retrieve books from predicted date of a query text reranked by similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-08T08:20:01.752894Z",
     "iopub.status.busy": "2023-11-08T08:20:01.752469Z",
     "iopub.status.idle": "2023-11-08T08:20:01.759510Z",
     "shell.execute_reply": "2023-11-08T08:20:01.758067Z",
     "shell.execute_reply.started": "2023-11-08T08:20:01.752865Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "#imports\n",
    "\n",
    "import pandas as pd\n",
    "import torch\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from simpletransformers.classification import ClassificationModel, ClassificationArgs\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "import gensim\n",
    "from gensim.models.doc2vec import TaggedDocument"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Read the dataset from our preprocessed json file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_json('gutenberg-dataset-v2.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Selecting our samples from the dataset and split it into train and test subsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>author</th>\n",
       "      <th>date</th>\n",
       "      <th>text_ratio</th>\n",
       "      <th>text</th>\n",
       "      <th>text_len_characters</th>\n",
       "      <th>weights</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>18261</th>\n",
       "      <td>The Piazza Tales</td>\n",
       "      <td>Herman Melville</td>\n",
       "      <td>1856</td>\n",
       "      <td>0.962312</td>\n",
       "      <td>huskily continued don benito painfully turning...</td>\n",
       "      <td>11497</td>\n",
       "      <td>0.025641</td>\n",
       "      <td>-0.28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38208</th>\n",
       "      <td>The Spell of Egypt</td>\n",
       "      <td>Robert Hichens</td>\n",
       "      <td>1911</td>\n",
       "      <td>0.970044</td>\n",
       "      <td>in the statue she is presented to us as a lime...</td>\n",
       "      <td>10691</td>\n",
       "      <td>0.071429</td>\n",
       "      <td>0.82</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>148107</th>\n",
       "      <td>Asbeïn: From the Life of a Virtuoso</td>\n",
       "      <td>Ossip Schubin</td>\n",
       "      <td>1890</td>\n",
       "      <td>0.960506</td>\n",
       "      <td>an upper story the maid goes happy to be relea...</td>\n",
       "      <td>11165</td>\n",
       "      <td>0.045455</td>\n",
       "      <td>0.40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33405</th>\n",
       "      <td>Sir Thomas More, or, Colloquies on the Progres...</td>\n",
       "      <td>Robert Southey</td>\n",
       "      <td>1824</td>\n",
       "      <td>0.974706</td>\n",
       "      <td>colloquy ivfeudal slaverygrowth of pauperism t...</td>\n",
       "      <td>11266</td>\n",
       "      <td>0.076923</td>\n",
       "      <td>-0.92</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>161091</th>\n",
       "      <td>Recollections of the Civil War / With the Lead...</td>\n",
       "      <td>Charles A. Dana</td>\n",
       "      <td>1863</td>\n",
       "      <td>0.968440</td>\n",
       "      <td>drawn the enemys attention to that quarter she...</td>\n",
       "      <td>10991</td>\n",
       "      <td>0.027027</td>\n",
       "      <td>-0.14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58607</th>\n",
       "      <td>The Ridin' Kid from Powder River</td>\n",
       "      <td>Henry Herbert Knibbs</td>\n",
       "      <td>1919</td>\n",
       "      <td>0.953091</td>\n",
       "      <td>that if the posse could see to shoot with such...</td>\n",
       "      <td>10520</td>\n",
       "      <td>0.018519</td>\n",
       "      <td>0.98</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>211406</th>\n",
       "      <td>La Ronge Journal, 1823</td>\n",
       "      <td>George Nelson</td>\n",
       "      <td>1823</td>\n",
       "      <td>0.956444</td>\n",
       "      <td>have mentioned first because as you may see i ...</td>\n",
       "      <td>10466</td>\n",
       "      <td>0.035714</td>\n",
       "      <td>-0.94</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>281414</th>\n",
       "      <td>Sketches in Holland and Scandinavia</td>\n",
       "      <td>Augustus J. C. Hare</td>\n",
       "      <td>1885</td>\n",
       "      <td>0.964674</td>\n",
       "      <td>sculptures and many most grand originals espec...</td>\n",
       "      <td>11528</td>\n",
       "      <td>0.076923</td>\n",
       "      <td>0.30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>170847</th>\n",
       "      <td>Rossa's Recollections, 1838 to 1898 / Childhoo...</td>\n",
       "      <td>Jeremiah O'Donovan Rossa</td>\n",
       "      <td>1838</td>\n",
       "      <td>0.963252</td>\n",
       "      <td>cut so deep a chasm he fell and bit the bloody...</td>\n",
       "      <td>10519</td>\n",
       "      <td>0.020833</td>\n",
       "      <td>-0.64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56695</th>\n",
       "      <td>The Papers and Writings of Abraham Lincoln — V...</td>\n",
       "      <td>Abraham Lincoln</td>\n",
       "      <td>1862</td>\n",
       "      <td>0.957157</td>\n",
       "      <td>divide his force sending part against each of ...</td>\n",
       "      <td>10920</td>\n",
       "      <td>0.020833</td>\n",
       "      <td>-0.16</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    title  \\\n",
       "18261                                    The Piazza Tales   \n",
       "38208                                  The Spell of Egypt   \n",
       "148107                Asbeïn: From the Life of a Virtuoso   \n",
       "33405   Sir Thomas More, or, Colloquies on the Progres...   \n",
       "161091  Recollections of the Civil War / With the Lead...   \n",
       "58607                    The Ridin' Kid from Powder River   \n",
       "211406                             La Ronge Journal, 1823   \n",
       "281414                Sketches in Holland and Scandinavia   \n",
       "170847  Rossa's Recollections, 1838 to 1898 / Childhoo...   \n",
       "56695   The Papers and Writings of Abraham Lincoln — V...   \n",
       "\n",
       "                          author  date  text_ratio  \\\n",
       "18261            Herman Melville  1856    0.962312   \n",
       "38208             Robert Hichens  1911    0.970044   \n",
       "148107             Ossip Schubin  1890    0.960506   \n",
       "33405             Robert Southey  1824    0.974706   \n",
       "161091           Charles A. Dana  1863    0.968440   \n",
       "58607       Henry Herbert Knibbs  1919    0.953091   \n",
       "211406             George Nelson  1823    0.956444   \n",
       "281414       Augustus J. C. Hare  1885    0.964674   \n",
       "170847  Jeremiah O'Donovan Rossa  1838    0.963252   \n",
       "56695            Abraham Lincoln  1862    0.957157   \n",
       "\n",
       "                                                     text  \\\n",
       "18261   huskily continued don benito painfully turning...   \n",
       "38208   in the statue she is presented to us as a lime...   \n",
       "148107  an upper story the maid goes happy to be relea...   \n",
       "33405   colloquy ivfeudal slaverygrowth of pauperism t...   \n",
       "161091  drawn the enemys attention to that quarter she...   \n",
       "58607   that if the posse could see to shoot with such...   \n",
       "211406  have mentioned first because as you may see i ...   \n",
       "281414  sculptures and many most grand originals espec...   \n",
       "170847  cut so deep a chasm he fell and bit the bloody...   \n",
       "56695   divide his force sending part against each of ...   \n",
       "\n",
       "        text_len_characters   weights  labels  \n",
       "18261                 11497  0.025641   -0.28  \n",
       "38208                 10691  0.071429    0.82  \n",
       "148107                11165  0.045455    0.40  \n",
       "33405                 11266  0.076923   -0.92  \n",
       "161091                10991  0.027027   -0.14  \n",
       "58607                 10520  0.018519    0.98  \n",
       "211406                10466  0.035714   -0.94  \n",
       "281414                11528  0.076923    0.30  \n",
       "170847                10519  0.020833   -0.64  \n",
       "56695                 10920  0.020833   -0.16  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df[df['date'] >= 1820]\n",
    "df = df[df['date'] <= 1920]\n",
    "\n",
    "earliest_date = df['date'].min()\n",
    "latest_date = df['date'].max()\n",
    "\n",
    "samples_list = []\n",
    "for year in range(earliest_date, latest_date + 1):\n",
    "    df_year = df[df['date'] == year]\n",
    "    if len(df_year) > 100:\n",
    "        samples = df_year.sample(100, random_state=42)\n",
    "        samples_list.append(samples)\n",
    "    else:\n",
    "        samples_list.append(df_year)\n",
    "\n",
    "df_samples = pd.concat(samples_list)\n",
    "df_samples['labels'] = df_samples['date'].apply(lambda date: np.interp(date, [earliest_date, latest_date], [-1, 1]))\n",
    "\n",
    "# Split data into train and test subsets\n",
    "df_train, df_test = train_test_split(df_samples, test_size=0.2, random_state=42, shuffle=True)\n",
    "df_test = df_test.head(10)\n",
    "df_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Setup and load the pretrained classification model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "cuda_available = torch.cuda.is_available()\n",
    "model_args = ClassificationArgs()\n",
    "model_args.reprocess_input_data = True\n",
    "model_args.overwrite_output_dir = True\n",
    "model_args.regression = True\n",
    "\n",
    "# Create a ClassificationModel\n",
    "bert_model = ClassificationModel(\n",
    "    'bert',\n",
    "    'bert-transformer/bert-base-historic-english-cased/outputs', # Load our own pre-trained model\n",
    "    num_labels=1,\n",
    "    args=model_args,\n",
    "    use_cuda=cuda_available\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Set up and train the Doc2Vec model for the re-ranking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_vectorize_docs(train_df, vector_size=40, min_count=4, epochs=30):\n",
    "    \"\"\"\n",
    "    Train a Doc2Vec model on a given DataFrame containing textual data and date information.\n",
    "\n",
    "    Parameters:\n",
    "    - train_df (pandas.DataFrame): Input DataFrame with 'text' and 'date' columns.\n",
    "    - vector_size (int): Dimensionality of the document vectors in the Doc2Vec model.\n",
    "    - min_count (int): Ignores all words with a total frequency lower than this.\n",
    "    - epochs (int): Number of iterations over the entire dataset during training.\n",
    "\n",
    "    Returns:\n",
    "    - pandas.DataFrame: DataFrame containing vectors, original text, date ranges, and other information.\n",
    "    \"\"\"\n",
    "\n",
    "    # Tokenization of each snippet\n",
    "    train_df['tokenized_text'] = train_df['text'].apply(word_tokenize)\n",
    "\n",
    "    new_df = train_df.groupby('date')['tokenized_text'].agg(lambda x: sum(x, [])).reset_index()                          # Groupe snippets by Year\n",
    "\n",
    "    # Preptaring the data for the\n",
    "    train_corpus = []\n",
    "    # Doc2Vec model\n",
    "    for i, words in enumerate(new_df['tokenized_text']):\n",
    "        tagged_doc = TaggedDocument(words=words, tags=[str(i)])\n",
    "        train_corpus.append(tagged_doc)\n",
    "\n",
    "    # Train the Doc2Vec model\n",
    "    model = gensim.models.Doc2Vec(vector_size=vector_size, min_count=min_count, epochs=epochs)\n",
    "    model.build_vocab(train_corpus)\n",
    "    model.train(train_corpus, total_examples=model.corpus_count, epochs=model.epochs)\n",
    "\n",
    "    # Infer Vectors and save in column (maybe usfull if we want to use them for further processing)\n",
    "    vectors = [model.infer_vector(words) for words in new_df['tokenized_text']]\n",
    "    new_df['vectors'] = vectors\n",
    "\n",
    "    return new_df, model\n",
    "\n",
    "def get_most_similar_books(query, model):\n",
    "    df_query = pd.DataFrame({'text': [query]})\n",
    "    df_query['tokenized_text'] = df_query['text'].apply(word_tokenize)\n",
    "    \n",
    "    vector = model.infer_vector(df_query['tokenized_text'].iloc[0])\n",
    "    \n",
    "    similar_docs = model.dv.most_similar([vector])\n",
    "    return similar_docs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Helper methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_date_from_prediction(prediction):\n",
    "    return int(np.interp(prediction, [-1, 1], [earliest_date, latest_date]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create predictions and retrieve book of the predicted year, then re-rank them by similarity to the query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/10 [00:00<?, ?it/s]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      " 10%|█         | 1/10 [00:05<00:46,  5.19s/it]\n",
      "100%|██████████| 2/2 [00:01<00:00,  1.65it/s]\n",
      "/var/folders/g4/qmhnsrqs0h7c0g47_37rh7qr0000gn/T/ipykernel_11766/3275198125.py:16: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  train_df['tokenized_text'] = train_df['text'].apply(word_tokenize)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('0', 0.9791324138641357)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/g4/qmhnsrqs0h7c0g47_37rh7qr0000gn/T/ipykernel_11766/3275198125.py:16: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  train_df['tokenized_text'] = train_df['text'].apply(word_tokenize)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('0', 0.9855530858039856)]\n"
     ]
    }
   ],
   "source": [
    "query_texts = df_test['text'].tolist()\n",
    "query_real_dates = df_test['date'].tolist()\n",
    "predictions, raw_outputs = bert_model.predict(query_texts)\n",
    "prediction_dates = [get_date_from_prediction(pred) for pred in predictions]\n",
    "\n",
    "for query, pred_date in zip(query_texts, prediction_dates):\n",
    "    df_books_from_pred_date = df[df['date'] == pred_date]\n",
    "    df_trained, doc2vec_model = train_and_vectorize_docs(df_books_from_pred_date)\n",
    "    similar_docs = get_most_similar_books(query, doc2vec_model)\n",
    "    print(similar_docs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
