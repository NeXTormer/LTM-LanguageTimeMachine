{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import T5Tokenizer, T5ForConditionalGeneration\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "Can't load tokenizer for 't5-3b'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 't5-3b' is the correct path to a directory containing all relevant files for a T5Tokenizer tokenizer.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[1;32m/Users/felix/Developer/Jupyter/ebook-ingester/transformer-test.ipynb Cell 2\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/felix/Developer/Jupyter/ebook-ingester/transformer-test.ipynb#W1sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m tokenizer \u001b[39m=\u001b[39m T5Tokenizer\u001b[39m.\u001b[39;49mfrom_pretrained(\u001b[39m\"\u001b[39;49m\u001b[39mt5-3b\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/felix/Developer/Jupyter/ebook-ingester/transformer-test.ipynb#W1sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m model \u001b[39m=\u001b[39m T5ForConditionalGeneration\u001b[39m.\u001b[39mfrom_pretrained(\u001b[39m\"\u001b[39m\u001b[39mt5-3b\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:1809\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *init_inputs, **kwargs)\u001b[0m\n\u001b[1;32m   1803\u001b[0m     logger\u001b[39m.\u001b[39minfo(\n\u001b[1;32m   1804\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mCan\u001b[39m\u001b[39m'\u001b[39m\u001b[39mt load following files from cache: \u001b[39m\u001b[39m{\u001b[39;00munresolved_files\u001b[39m}\u001b[39;00m\u001b[39m and cannot check if these \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1805\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mfiles are necessary for the tokenizer to operate.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1806\u001b[0m     )\n\u001b[1;32m   1808\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mall\u001b[39m(full_file_name \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mfor\u001b[39;00m full_file_name \u001b[39min\u001b[39;00m resolved_vocab_files\u001b[39m.\u001b[39mvalues()):\n\u001b[0;32m-> 1809\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mEnvironmentError\u001b[39;00m(\n\u001b[1;32m   1810\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mCan\u001b[39m\u001b[39m'\u001b[39m\u001b[39mt load tokenizer for \u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00mpretrained_model_name_or_path\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m. If you were trying to load it from \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1811\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m'\u001b[39m\u001b[39mhttps://huggingface.co/models\u001b[39m\u001b[39m'\u001b[39m\u001b[39m, make sure you don\u001b[39m\u001b[39m'\u001b[39m\u001b[39mt have a local directory with the same name. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1812\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mOtherwise, make sure \u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00mpretrained_model_name_or_path\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m is the correct path to a directory \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1813\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mcontaining all relevant files for a \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mcls\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m tokenizer.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1814\u001b[0m     )\n\u001b[1;32m   1816\u001b[0m \u001b[39mfor\u001b[39;00m file_id, file_path \u001b[39min\u001b[39;00m vocab_files\u001b[39m.\u001b[39mitems():\n\u001b[1;32m   1817\u001b[0m     \u001b[39mif\u001b[39;00m file_id \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m resolved_vocab_files:\n",
      "\u001b[0;31mOSError\u001b[0m: Can't load tokenizer for 't5-3b'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 't5-3b' is the correct path to a directory containing all relevant files for a T5Tokenizer tokenizer."
     ]
    }
   ],
   "source": [
    "tokenizer = T5Tokenizer.from_pretrained(\"t5-3b\")\n",
    "model = T5ForConditionalGeneration.from_pretrained(\"t5-3b\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GERMAN AUTHORS. VOL. 15. THE DEAD LAKE &\n"
     ]
    }
   ],
   "source": [
    "snippet = \"\"\" \n",
    "Transcriber's . Page scan source: http://www.archive.org/details/deadlakeothertal00heys COLLECTION OF GERMAN AUTHORS. VOL. 15. THE DEAD LAKE & OTHER TALES BY PAUL HEYSE. IN ONE VOLUME. THE DEAD LAKE AND OTHER TALES BY PAUL HEYSE. FROM THE GERMAN BY BY MARY WILSON. Authorized Edition . LEIPZIG 1870 BERNHARD TAUCHNITZ. LONDON: SAMPSON LOW, SON, AND MARSTON. CROWN BUILDINGS, 188, FLEET STREET. PARIS: C. REINWALD, 15, RUE DES SAINTS PÃˆRES. CONTENTS. A FORTNIGHT AT THE\n",
    "DEAD LAKE DOOMED BEATRICE BEGINNING, AND END A FORTNIGHT AT THE DEAD LAKE. THE DEAD LAKE. Summer was at its heighth, yet in one corner of the Alps an icy col\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "input_ids = tokenizer(\"What year number is mentioned in this text, give only the number: \" + snippet, return_tensors=\"pt\").input_ids\n",
    "outputs = model.generate(input_ids)\n",
    "print(tokenizer.decode(outputs[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
